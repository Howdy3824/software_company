name: Doctests

on:
  push:
    branches:
      - doctest*
  repository_dispatch:
  schedule:
    - cron: "0 0 * * *"


env:
  HF_HOME: /mnt/cache
  TRANSFORMERS_IS_CI: yes
  RUN_SLOW: yes
  OMP_NUM_THREADS: 16
  MKL_NUM_THREADS: 16
  SIGOPT_API_TOKEN: ${{ secrets.SIGOPT_API_TOKEN }}
  TF_FORCE_GPU_ALLOW_GROWTH: true

jobs:
  run_doctests:
    runs-on: [self-hosted, doc-tests-gpu]
    container:
      image: huggingface/transformers-all-latest-gpu
      options: --gpus 0 --shm-size "16gb" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/
    steps:
      - uses: actions/checkout@v2
        with:
          repository: 'huggingface/transformers'
          path: transformers

      - name: NVIDIA-SMI
        run: |
          nvidia-smi

      - name: GPU visibility
        working-directory: transformers
        run: |
          utils/print_env_pt.py
          TF_CPP_MIN_LOG_LEVEL=3 python3 -c "import tensorflow as tf; print('TF GPUs available:', bool(tf.config.list_physical_devices('GPU')))"
          TF_CPP_MIN_LOG_LEVEL=3 python3 -c "import tensorflow as tf; print('Number of TF GPUs available:', len(tf.config.list_physical_devices('GPU')))"

      - name: Prepare files for doctests
        working-directory: transformers
        run: |
          python3 utils/prepare_for_doc_test.py src docs

      - name: Run doctests
        working-directory: transformers
        run: |
          python3 -m pytest --doctest-modules $(cat utils/documentation_tests.txt) -sv --doctest-continue-on-failure --doctest-glob="*.mdx"

      - name: Clean files after doctests
        working-directory: transformers
        run: |
          python3 utils/prepare_for_doc_test.py src docs --remove_new_line
